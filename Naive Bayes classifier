import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal as mvn #(for stats methods like bayes)


data = pd.read_csv("/Users/jbhatt/Documents/exNB.csv",header=None)
#print(data.head(5))
#print(data.shape)


X = data.to_numpy() #turn into numpy array
y = X[:,-1] #y is lables, this shorthand means everysingle row in the last column 
X = X[:,:-1] # X is every row in every column except the last one

#print(y.shape)

###graphing###
#weight
plt.figure()
plt.hist(X[y == 1,0],label='Male',alpha=0.5,bins=30) #histogram of X where y is 0 or 1
plt.hist(X[y == 0,0],label = 'Female',alpha=0.5,bins=30)
plt.legend()

#height
plt.figure()
plt.hist(X[y==1,1],label = "Male",alpha=0.8)
plt.hist(X[y==0],label = "Female",alpha=0.25)
plt.legend()

plt.figure()
plt.scatter(X[:,0],X[:,1],c=y,alpha=0.25)#all the data in x axis and Y axis

#building classifier

class GaussNB():
    def fit(self, X, y, epsilon = 1e-3):
        self.likelyhoods = dict()
        self.priors = dict()

        self.K = set(y.astype(int))#a range of ints of unique items like 0 to 7

        for k in self.K:
            X_k = X[y==k,:]
            self.likelyhoods[k] = {'mean':X_k.mean(axis=0),'variance':X_k.var(axis=0)+ epsilon}
            self.priors[k] = len(X_k)/len(X) # number of class /total number
    
    def predict(self, X):

        N,D = X.shape
        P_hat = np.zeros((N,len(self.K)))
        for k,l in self.likelyhoods.items():
            P_hat[:,k] = mvn.logpdf(X,l["mean"],l["variance"])+np.log(self.priors[k])
        return P_hat.argmax(axis=1)
            



       


gnb = GaussNB()
gnb.fit(X,y)
y_hat = gnb.predict(X)

def accuracy(y,y_hat):
    return np.mean(y== y_hat)

print(accuracy(y,y_hat))

class BernNB():
  def fit(self, X, y, epsilon = 1e-10):
    N, D = X.shape
    self.likelihoods = {}
    self.priors = {}
    self.K = set(y.astype(int))

    for k in self.K:
      X_k = X[y==k,:]
      p = (sum(X_k)+1) / (len(X_k)+2)
      self.likelihoods[k] = {'mean': p, 'cov': p * (1 - p) + epsilon}
      self.priors[k] = len(X_k)/len(X)

  def predict(self, X):
    N, D = X.shape
    P_hat = np.zeros((N, len(self.K)))

    for k,l in self.likelihoods.items():
      # Using the Bernoulli funtion/formula. Trick is to get the matrices/vectors to go from mxn to a 1x1 number for each k value.
      P_hat[:,k] = np.log(self.priors[k]) + np.matmul(X, np.log(l['mean'])) + np.matmul((1 - X), np.log(abs(1-l['mean'])))

    return P_hat.argmax(axis =1)
    
bnb = BernNB()
bnb.fit(X_train,y_train) # Use the X and Y Training set here
y_hat = bnb.predict(X_test) # Use the X Test here
print(accuracy(y_test, y_hat))    

#not naive bayes
class GaussBayes():
    def fit(self, X, y, epsilon = 1e-3):
        self.likelihoods = dict()
        self.priors = dict()
        sefl.K = set(y.astype(int))

        for k in self.K:
            X_k = X[y == k,:]
            N_k, D = X_k.shape
            mu_k == X_k.mean(axis=0)
            self.likelihoods[k] = {"mean" : X_k.mean(axis=0),"cov":
                                  (1/(N_k-1))*np.matmul(X_k-mu_k.T,X_k - mu_k)+ epsilon*np.identity(D) }
    
   def predict(self, X):
        N,D = X.shape
        P_hat = np.zeros((N, len(self.K)))

        for k,l in self.likelihoods.items():
            P_hat[:,k] = mvn.logpdf(X, l["mean"], l["cov"])+np.log(self.priors[k])

        return P_hat.argmax(axis=1)
