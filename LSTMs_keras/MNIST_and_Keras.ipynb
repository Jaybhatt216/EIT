{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST and Keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNygCLPPMDuvJFy1oORDMaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaybhatt216/EIT/blob/main/MNIST_and_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K361HGaH81dl"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential, load_model\r\n",
        "from keras.layers import Dense, Embedding, LSTM, Bidirectional\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from keras.utils.np_utils import to_categorical\r\n",
        "\r\n",
        "import tensorflow.compat.v2 as tf\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "\r\n",
        "tf.enable_v2_behavior()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-DDhySk9AHJ"
      },
      "source": [
        "## Step 1: Create your input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3q7mX5C86sa"
      },
      "source": [
        "#get the MNIST data set and perform train test split \r\n",
        "\r\n",
        "(ds_train, ds_test), ds_info = tfds.load(\r\n",
        "    'mnist',\r\n",
        "    split=['train', 'test'],\r\n",
        "    shuffle_files=True,\r\n",
        "    as_supervised=True,\r\n",
        "    with_info=True,\r\n",
        ")\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1ahUfQ09vR0"
      },
      "source": [
        "## Build training pipeline\r\n",
        "## Apply the following transormations:\r\n",
        "\r\n",
        "## ds.map: TFDS provide the images as tf.uint8, while the model expect tf.float32, so normalize images\r\n",
        "##ds.cache As the dataset fit in memory, cache before shuffling for better performance.\r\n",
        "##Note: Random transformations should be applied after caching\r\n",
        "##ds.shuffle: For true randomness, set the shuffle buffer to the full dataset size.\r\n",
        "##Note: For bigger datasets which do not fit in memory, a standard value is 1000 if your system allows it.\r\n",
        "##ds.batch: Batch after shuffling to get unique batches at each epoch.\r\n",
        "##ds.prefetch: Good practice to end the pipeline by prefetching for performances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBpxgNXa9l2f"
      },
      "source": [
        "#converting or normalizing from unit8 to float32\r\n",
        "\r\n",
        "def normalize_img(image, label):\r\n",
        "  return tf.cast(image, tf.float32) / 255., label\r\n",
        "\r\n",
        "ds_train = ds_train.map(\r\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
        "ds_train = ds_train.cache()\r\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\r\n",
        "ds_train = ds_train.batch(128)\r\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "\r\n",
        "#data AUTOTUNE. prefetch transformation, which can be used to decouple the time \r\n",
        "#when data is produced from the time when data is consumed. ... \r\n",
        "#In particular, the transformation uses a background thread and an internal buffer to prefetch elements \r\n",
        "#from the input dataset ahead of the time they are requested.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#cache transformation can cache a dataset, either in memory or on local storage. \r\n",
        "#This will save some operations (like file opening and data reading) \r\n",
        "#from being executed during each epoch.\r\n",
        "\r\n",
        "\r\n",
        "#Dataset.prefetch() method that makes it easier to add prefetching at \r\n",
        "#any point in the pipeline, not just after a map()\r\n",
        "#For example, Dataset.prefetch() will start a background thread to populate a \r\n",
        "#ordered buffer that acts like a tf.FIFOQueue, so that downstream pipeline stages need not block. \r\n",
        "#However, the prefetch() implementation is much simpler, \r\n",
        "#because it doesn't need to support as many different concurrent operations as a tf.FIFOQueue."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C61ccVZc_1Sh"
      },
      "source": [
        "## Build evaluation pipeline\r\n",
        "## Testing pipeline is similar to the training pipeline, with small differences:\r\n",
        "\r\n",
        "## No ds.shuffle() call\r\n",
        "## Caching is done after batching (as batches can be the same between epoch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Ua1qn1_vgM"
      },
      "source": [
        "\r\n",
        "\r\n",
        "ds_test = ds_test.map(\r\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
        "ds_test = ds_test.batch(128)\r\n",
        "ds_test = ds_test.cache()\r\n",
        "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8hVmr7cAf_9"
      },
      "source": [
        "## Step 2: Create and train the model\r\n",
        "## Plug the input pipeline into Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7Hzre9LAZzZ",
        "outputId": "62b8f243-5e52-4301-8602-e984903c88dc"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n",
        "  tf.keras.layers.Dense(128,activation='relu'),\r\n",
        "  tf.keras.layers.Dense(10)\r\n",
        "])\r\n",
        "model.compile(\r\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\r\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\r\n",
        ")\r\n",
        "\r\n",
        "model.fit(\r\n",
        "    ds_train,\r\n",
        "    epochs=6,\r\n",
        "    validation_data=ds_test,\r\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "469/469 [==============================] - 9s 6ms/step - loss: 0.5958 - sparse_categorical_accuracy: 0.8420 - val_loss: 0.1933 - val_sparse_categorical_accuracy: 0.9435\n",
            "Epoch 2/6\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1730 - sparse_categorical_accuracy: 0.9525 - val_loss: 0.1323 - val_sparse_categorical_accuracy: 0.9604\n",
            "Epoch 3/6\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1190 - sparse_categorical_accuracy: 0.9653 - val_loss: 0.1105 - val_sparse_categorical_accuracy: 0.9675\n",
            "Epoch 4/6\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0928 - sparse_categorical_accuracy: 0.9737 - val_loss: 0.0967 - val_sparse_categorical_accuracy: 0.9698\n",
            "Epoch 5/6\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0715 - sparse_categorical_accuracy: 0.9795 - val_loss: 0.0898 - val_sparse_categorical_accuracy: 0.9726\n",
            "Epoch 6/6\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0596 - sparse_categorical_accuracy: 0.9824 - val_loss: 0.0790 - val_sparse_categorical_accuracy: 0.9765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f09712d7358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYQwSvIoAsyG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}