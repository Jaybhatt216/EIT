{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAjAdaQCUakGbc8HTz6H33",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaybhatt216/EIT/blob/main/LSTM_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwsgJ2C_Fdtv"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import torch "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksXa-IwPHKeT"
      },
      "source": [
        "## Build layer class\r\n",
        "we will build a layer class on top of super class torch.nn.Module "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBWp4JwIHD9X"
      },
      "source": [
        "class Layer(torch.nn.Module):\r\n",
        "  #size in is size before size out is out for this layer\r\n",
        "  #nn parameter is how you feed the neural net\r\n",
        "  def __init__(self,size_in,size_out,activation):\r\n",
        "    super(Layer,self).__init__()\r\n",
        "    self.weights = torch.nn.Parameter(torch.randn(size_in,size_out,requires_grad=True))\r\n",
        "    self.bias = torch.nn.Parameter(torch.randn(1,size_out,requires_grad=True))\r\n",
        "    self.activation = activation\r\n",
        "\r\n",
        "\r\n",
        "  def Forward(self, z_in):\r\n",
        "    return self.activation(z_in @self.weights + self.bias) \r\n",
        "\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGxNhELtnJ3H"
      },
      "source": [
        "# Layer initalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNm3M_4qJJgO"
      },
      "source": [
        "forget = Layer(38,15,torch.nn.Sigmoid())\r\n",
        "loss_func = torch.nn.MSELoss()\r\n",
        "opt = torch.optim.Adam(forget.parameters())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9FdZ9dwoGNM",
        "outputId": "f56434d9-eeaa-492a-bf77-93bd976467fd"
      },
      "source": [
        "x_in = torch.randn(1,38)\r\n",
        "y = torch.rand(1,15)\r\n",
        "print(x_in)\r\n",
        "print(y)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.8628, -0.2809,  0.6315,  0.4143,  0.0128,  0.6250, -1.7685, -0.4364,\n",
            "         -1.2858,  0.9515,  0.2903,  0.4718,  2.3356, -0.3511,  0.6788,  1.4555,\n",
            "         -0.4935, -0.8775,  0.7540,  0.2212,  0.9125, -0.8637, -0.6678,  0.8865,\n",
            "          0.9926, -0.5239, -0.7671, -1.2793,  0.3747,  0.6412, -0.3202, -0.0684,\n",
            "          1.0640,  1.1321,  0.0516, -1.6982,  0.5919, -0.6233]])\n",
            "tensor([[0.3317, 0.5169, 0.2276, 0.0018, 0.2405, 0.3352, 0.8150, 0.4871, 0.7911,\n",
            "         0.3639, 0.6621, 0.7528, 0.0642, 0.0076, 0.3080]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISrxKGYvn2xT",
        "outputId": "930ffc3f-b324-474d-8577-c74b25828eae"
      },
      "source": [
        "print(forget.bias)\r\n",
        "out = forget.Forward(x_in)\r\n",
        "loss = loss_func(out,y)\r\n",
        "loss.backward()\r\n",
        "opt.step()\r\n",
        "opt.zero_grad()\r\n",
        "print(forget.bias)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-1.7523,  0.9407, -0.7421,  1.6027,  0.6439, -1.1389,  0.7855,  0.8710,\n",
            "         -0.6491,  0.0872,  0.5905, -1.3373, -1.3933, -0.2294,  0.2139]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-1.7513,  0.9407, -0.7411,  1.6017,  0.6429, -1.1399,  0.7845,  0.8720,\n",
            "         -0.6481,  0.0862,  0.5915, -1.3383, -1.3923, -0.2304,  0.2129]],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMsILFr1pUB4"
      },
      "source": [
        "# RNN recurrent neural network\r\n",
        "retains some info frome every stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bdfilUvo3e2"
      },
      "source": [
        "class RNN(torch.nn.Module):\r\n",
        "  #size of memory is amount of data you will save layer to layer\r\n",
        "  #only have 2 hidden layers memory and out\r\n",
        "  def __init__(self,size_in,size_out,size_memory):\r\n",
        "    super(RNN,self).__init__()\r\n",
        "    self.size_memory = size_memory\r\n",
        "    self.size_memory_layer = Layer(size_in + size_memory, size_memory, torch.tanh) #previous layers plus new data\r\n",
        "    self.out_layer = Layer(size_memory, size_out, torch.sigmoid)\r\n",
        "\r\n",
        "  def Forward(self,X):\r\n",
        "    memory = torch.zeros(1,self.size_memory)\r\n",
        "    y_hat = []\r\n",
        "    for i in range(X.shape[0]):\r\n",
        "      x_in = X[[i],:]\r\n",
        "      z_in = torch.cat([x_in,memory], dim=1)\r\n",
        "      memory = self.size_memory_layer.Forward(z_in)\r\n",
        "      y_hat.append(self.out_layer.Forward(memory))\r\n",
        "\r\n",
        "    return torch.cat(y_hat,dim=0)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rxQE30nsaAY"
      },
      "source": [
        "rnn = RNN(38,15,5)\r\n",
        "loss_func = torch.nn.MSELoss()\r\n",
        "opt = torch.optim.Adam(rnn.parameters())\r\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egR0VeWZvvSL",
        "outputId": "ee61c20e-4e15-4dc1-b1b2-f79591d4cdf9"
      },
      "source": [
        "print(rnn.size_memory_layer.bias)\r\n",
        "y_hat = rnn.Forward(x_in)\r\n",
        "loss = loss_func(y_hat,y)\r\n",
        "loss.backward()\r\n",
        "opt.step()\r\n",
        "opt.zero_grad\r\n",
        "print()\r\n",
        "print(rnn.size_memory_layer.bias)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.5963, -0.1294,  0.4803,  0.3810, -0.9185]], requires_grad=True)\n",
            "\n",
            "Parameter containing:\n",
            "tensor([[ 0.5973, -0.1284,  0.4794,  0.3820, -0.9185]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djHsLhJ0x4bV"
      },
      "source": [
        "# Build LSTM: Long Short Term Memory "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RLEawMewivx"
      },
      "source": [
        "class LSTM(torch.nn.Module):\r\n",
        "  #2 memory size long and short\r\n",
        "  def __init__(self, size_in, size_out, size_long, size_short):\r\n",
        "    super(LSTM, self).__init__()\r\n",
        "    self.size_long = size_long\r\n",
        "    self.size_short = size_short\r\n",
        "\r\n",
        "    size_z = (size_in + size_short)\r\n",
        "\r\n",
        "    #build series of layers\r\n",
        "\r\n",
        "    #forget_gate layer\r\n",
        "    self.forget_gate = Layer(size_z, size_long, torch.sigmoid)\r\n",
        "\r\n",
        "    #memory gate layer\r\n",
        "    self.memory_gate = Layer(size_z, size_long, torch.sigmoid)\r\n",
        "\r\n",
        "    #memory layer\r\n",
        "    self.memory_layer = Layer(size_z, size_long, torch.tanh)\r\n",
        "\r\n",
        "    #recall gate \r\n",
        "    self.recall_gate = Layer(size_z, size_short, torch.sigmoid)\r\n",
        "    #recall layer\r\n",
        "    self.recall_layer = Layer(size_long, size_short, torch.tanh)\r\n",
        "\r\n",
        "    #output layer\r\n",
        "    self.output_gate = Layer(size_short, size_out, torch.sigmoid)\r\n",
        "\r\n",
        "  def Forward(self, X):\r\n",
        "    mem_short = torch.zeros(1,self.size_short)\r\n",
        "    mem_long =  torch.zeros(1,self.size_long)\r\n",
        "    y_hat = []\r\n",
        "    for i in range(X.shape[0]): #for all the rows\r\n",
        "      X_t = X[[i],:]\r\n",
        "      z_t = torch.cat([X_t,mem_short], dim=1)\r\n",
        "      mem_long = mem_long*(self.forget_gate.Forward(z_t))\r\n",
        "      mem_long = mem_long+(self.memory_gate.Forward(z_t)*self.memory_layer.Forward(z_t))\r\n",
        "      mem_short = self.recall_gate.Forward(z_t)+self.recall_layer.Forward(mem_long)\r\n",
        "\r\n",
        "\r\n",
        "      y_hat.append(self.output_gate.Forward(mem_short))\r\n",
        "\r\n",
        "    return torch.cat(y_hat, dim=0)\r\n",
        "\r\n",
        "  def Generate(self, start,stop,random_factor):\r\n",
        "    y_hat = [start]\r\n",
        "    mem_long = torch.randn(1, self.size_long, random_factor)\r\n",
        "    mem_short = torch.randn(1, self.size_short, random_factor)\r\n",
        "\r\n",
        "    while (y_hat[-1] != stop).any() and len(y_hat)<30:\r\n",
        "      x_t = y_hat[-1]\r\n",
        "      z_t =  torch.cat([x_t, mem_short], dim=1)\r\n",
        "      mem_long = mem_long * self.forget_gate.Forward(z_t)\r\n",
        "      mem_long = mem_long + (self.memory_gate.Forward(z_t)*self.memory_layer.Forward(z_t))\r\n",
        "      mem_short = self.recall_gate.Forward(z_t)+self.recall_layer.Forward(mem_long)\r\n",
        "      out = self.output_gate.Forward(mem_short)\r\n",
        "      out = torch.argmax(out, dim=1)\r\n",
        "      y_hat.append(torch.zeros(stop.shape))\r\n",
        "      y_hat[-1][0,out]= 1\r\n",
        "    return torch.cat(y_hat,dim=0)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwBKAo4a2x4o"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpyKx8ac63bm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}