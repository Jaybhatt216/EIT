{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q_learning_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2I5SCh3QZhF3NXHVKDtas",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaybhatt216/EIT/blob/main/Q_learning_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"
        "\n",
        "Reinforcement Q-Learning\n",
        "- Teach a Taxi to pick up and drop off passengers at the right locations with Reinforcement Learning\n",
        "- Example Design: Self-Driving Cab\n",
        "\n",
        "- Drop off the passenger to the right location.\n",
        "- Save passenger's time by taking minimum time possible to drop off\n",
        "- Take care of passenger's safety and traffic rules"
      ],
      "metadata": {
        "id": "XLHe6QQO7Za8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOZP5U2d3fQT",
        "outputId": "b00b0ed0-c031-43d5-c53f-f79516dee9b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.22.5)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# State Space:\n",
        "In Reinforcement Learning, the agent encounters a state, and then takes action according to the state it's in.\n",
        "\n",
        "The State Space is the set of all possible situations our taxi could inhabit. The state should contain useful information the agent needs to make the right action.\n",
        "\n",
        "Let's say we have a training area for our Smartcab where we are teaching it to transport people in a parking lot to four different locations (R, G, Y, B):"
      ],
      "metadata": {
        "id": "zAb3sv2q72Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zzvw1ZD3mie",
        "outputId": "4e28f53f-835e-4673-9c3c-f4a04cd0e7de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Action Space\n",
        "The agent encounters one of the 500 states and it takes an action. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger.\n",
        "\n",
        "In other words, we have six possible actions:\n",
        "\n",
        "1. south\n",
        "2. north\n",
        "3. east\n",
        "4. west\n",
        "5. pickup\n",
        "6. dropoff"
      ],
      "metadata": {
        "id": "1lzrrCXx8Rw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset() # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTtK9hS_3016",
        "outputId": "901996df-d245-46f3-90b8-29217d06d672"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. \n",
        " - We receive +20 points for a successful drop-off \n",
        " - lose 1 point for every time-step it takes. \n",
        " - There is also a 10 point penalty for illegal pick-up and drop-off actions."
      ],
      "metadata": {
        "id": "p-6Zc8vH82vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
        "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
        "- R, G, Y, B are the possible pickup and destination locations. \n",
        "- The blue letter represents the current passenger pick-up location, \n",
        "- The purple letter is the current destination.\n",
        "\n",
        "# Identify a state uniquely by assigning a unique number to every possible state RL learns to choose an action number from 0-5 where:\n",
        "\n",
        "- 0 = south\n",
        "- 1 = north\n",
        "- 2 = east\n",
        "- 3 = west\n",
        "- 4 = pickup\n",
        "- 5 = dropoff"
      ],
      "metadata": {
        "id": "dsSlo2X79L2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtK8G3cG4WH0",
        "outputId": "f8f9ad1e-af4a-4c9a-97dd-5839549b997e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Reward Table\n",
        "When the Taxi environment is created, there is an initial Reward table that's also created, called `P`.\n",
        "\n",
        "Since every state is in this matrix, we can see the default reward values assigned to our illustration's state"
      ],
      "metadata": {
        "id": "Ag_pvjtc9_1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.P[328]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxxNioGY4aMe",
        "outputId": "f1c75838-f7e3-4df5-b2f1-0d2248858715"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# brute-force our way to solving the problem without RL."
      ],
      "metadata": {
        "id": "XfUX7tqN-UfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.s = 328  # set environment to illustration's state\n",
        "\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "\n",
        "frames = [] # for animation\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    \n",
        "    # Put each rendered frame into dict for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "    \n",
        "    \n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwkhC14U4dan",
        "outputId": "ea1b4e05-4405-4b2b-efd5-ea4038f38407"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timesteps taken: 1241\n",
            "Penalties incurred: 399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "print_frames(frames)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "wRW-epaG4iMi",
        "outputId": "04886403-474e-458f-a692-9a8af48e5312"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "Timestep: 259\n",
            "State: 348\n",
            "Action: 4\n",
            "Reward: -10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-adcf58a03b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-adcf58a03b7e>\u001b[0m in \u001b[0;36mprint_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Action: {frame['action']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reward: {frame['reward']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Not good. Our agent takes thousands of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination"
      ],
      "metadata": {
        "id": "6rnKWPg1-esD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-learning\n",
        "Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
        "\n",
        "In our Taxi environment, we have the reward table, P, that the agent will learn from. It does thing by looking receiving a reward for taking an action in the current state, then updating a Q-value to remember if that action was beneficial.\n",
        "\n",
        "The values store in the Q-table are called a Q-values, and they map to a (state, action) combination.\n",
        "\n",
        "A Q-value for a particular state-action combination is representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards.\n",
        "\n",
        "For example, if the taxi is faced with a state that includes a passenger at its current location, it is highly likely that the Q-value for pickup is higher when compared to other actions, like dropoff or north.\n",
        "\n",
        "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:"
      ],
      "metadata": {
        "id": "BvoyZROo-tWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "metadata": {
        "id": "bgYnMHKR5Npj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) \n",
        "        \n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        \n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72SnYFWB5cSg",
        "outputId": "76db1c7a-930b-47de-8293-b7ae9da62c51"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 1min 4s, sys: 11 s, total: 1min 15s\n",
            "Wall time: 1min 7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table[328]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB80xtPq5fqF",
        "outputId": "7f46c351-3ec8-4f62-b52d-e35eaddbe023"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.39610919,  -2.27325184,  -2.41121717,  -2.35360139,\n",
              "       -10.90719107, -10.5901238 ])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjB5xgO86NAq",
        "outputId": "26044502-cfc6-4937-b65e-74add84c4126"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.98\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "poN9VB2a6SoE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
