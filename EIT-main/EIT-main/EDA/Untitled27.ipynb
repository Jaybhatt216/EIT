{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled27.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XH3TTxrcEpt"
      },
      "source": [
        "# Analysis and Dataframes\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# tensofrflow\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76xZwuZkciCI"
      },
      "source": [
        "|# import data\r\n",
        "shakespear = pd.read_csv('some csv file.csv')\r\n",
        "# example ['hello there', 'General Kenobi']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPlMEVV8dNwm"
      },
      "source": [
        "wikipedia_article = \"\"\"\r\n",
        "The algorithm was developed by Google for use in machine translation.[2]\r\n",
        "\r\n",
        "In 2019, Facebook announced its use in symbolic integration and resolution of differential equations. The company claimed that it could solve complex equations more rapidly and with greater accuracy than commercial solutions such as Mathematica, MATLAB and Maple. First, the equation is parsed into a tree structure to avoid notational idiosyncrasies. An LSTM neural network then applies its standard pattern recognition facilities to process the tree.[3]\r\n",
        "\r\n",
        "In 2020, Google released Meena, a 2.6 billion parameter seq2seq-based chatbot trained on a 341 GB data set. Google claimed that the chatbot has 1.7 times greater model capacity than OpenAI's GPT-2,[4] whose May 2020 successor, the 175 billion parameter GPT-3, trained on a \"45TB dataset of plaintext words (45,000 GB) that was ... filtered down to 570 GB.\"\r\n",
        "\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3lgOH4PdvhP"
      },
      "source": [
        "### Data Cleaning (Preprocessing)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "TpVwc7gld_T-",
        "outputId": "cb7cbafe-3be3-4199-8235-7dfc11048914"
      },
      "source": [
        "### Create a Tokenizer\r\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer\r\n",
        "\r\n",
        "# tokenizer.fit_on_texts(wikipedia_article)\r\n",
        "\r\n",
        "# Explore the Data\r\n",
        "tokenizer.word_counts()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3de333d3c6c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwikipedia_article\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Explore the Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: fit_on_texts() missing 1 required positional argument: 'texts'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcHJ7Y-GkqWN"
      },
      "source": [
        "# Setting up the data\r\n",
        "\"\"\"[[1,2],\r\n",
        "    [2,3],\r\n",
        "    [3,4]]\"\"\"\r\n",
        "\r\n",
        "# Create "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRrRJ1yneGsC"
      },
      "source": [
        "# The model\r\n",
        "model = tf.keras.models.Sequential()\r\n",
        "# Input layer here (embedding will go here) (hint: (1,) )\r\n",
        "model.add(tf.keras.layers.Dense(100, activation='tanh'))\r\n",
        "model.add(tf.keras.layers.Dropout(0.1)) # Dropout \r\n",
        "model.add(tf.keras.layers.Dense(3, activation='tanh')) #Extract too see a visual!\r\n",
        "model.add(tf.keras.layers.Dense(1)) # ouput layer\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21S5xsDcf-PO"
      },
      "source": [
        "model.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}